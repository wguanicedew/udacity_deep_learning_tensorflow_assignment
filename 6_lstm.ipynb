{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "97\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "print(vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "    # print(batch_size)\n",
    "    # print(segment)\n",
    "    # print(self._cursor)\n",
    "    # print(self._last_batch)\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  # print(probabilities)\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.290488 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.86\n",
      "================================================================================\n",
      "rai  mvklbbb own esgfaaph ugdlee acqsxtnecjcmmubxt ydf tkmz kfbxi   n la rcmesld\n",
      "mi fu znp sm  uvcu c rahtxaxeirgtairw nigtettmh erduvgjax csls ehsp  iutc anstre\n",
      "kaerhq ea gyeihve  bhgnj e frjolcq ieslpio tzphlmrediapybrrwurhyerolzds oshfvdll\n",
      "raeajgutzfdoulouzleiyktauuevoe aruoqwxd eqveruaaqsrseqaas bwg lfgcoeond zwtuwaty\n",
      "c  qurefdebkar  io ypvub axebcertslvgfbipaqacyec to eepqebqetnrgpntuglxoavuqsgjs\n",
      "================================================================================\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 100: 2.594895 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.00\n",
      "Validation set perplexity: 10.63\n",
      "Average loss at step 200: 2.244327 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.56\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 300: 2.097418 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 400: 1.998698 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.58\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 500: 1.935082 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 600: 1.905499 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 700: 1.854329 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 800: 1.812670 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 900: 1.823073 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1000: 1.822651 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "================================================================================\n",
      "s ple reapeter undyarilatie in the  asevice to becaation of the tress infremor a\n",
      "litters with to over in the firyd fallem on theire the eare whate prefigising ne\n",
      "je lygreds he of there creeter s a meciog cont placuce le us soar liver but in t\n",
      "utisc and reland enchen earment overceies in earces geecadion sele is nived the \n",
      "itf bert in lowe who gmetce cestentes anderean formgie par chare heeder yamonisa\n",
      "================================================================================\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1100: 1.773353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1200: 1.748808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1300: 1.729359 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1400: 1.741362 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1500: 1.734484 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1600: 1.744323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.707506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1800: 1.669580 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 1900: 1.642341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2000: 1.692223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "chist ording if the impoint were in on the americi is is totnal alsobovher of te\n",
      "imian on things also wo accians last culficlus he digined d one nine zero one ni\n",
      "on get mining as the seite chirfed gho on hosws s dicition regidizicle carition \n",
      "z bopy impurtions bicgezrork university of ithers including in a vinisms replayi\n",
      "x cerlandeus of pirages were go downes untereven when woulds weopi bublial ruagb\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2100: 1.680034 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2200: 1.675160 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2300: 1.635033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2400: 1.655992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2500: 1.682110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2600: 1.650061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2700: 1.652667 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800: 1.648968 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.649146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3000: 1.645421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "ide one sign and bodding transticary k one nine eight two is thesed or all logiz\n",
      "used dorly mays prite was of n polating one nine flate indalma brounds of areway\n",
      "vel to one seven nine six two filations diadliuntm manianscemler bur yearliaberm\n",
      "je called in its percil first cash were a phote in reperance hispory and obatle \n",
      "j five the hat falty colarches an land fadeical are halt for  part eleo the elec\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3100: 1.621093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3200: 1.638793 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3300: 1.632480 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3400: 1.661669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3500: 1.654379 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3600: 1.665039 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3700: 1.644435 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3800: 1.640011 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3900: 1.633662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4000: 1.647809 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "der by distuirent honsitions the razined with diames james casts the has alit in\n",
      "xing zero th to tree and extected usiban speciulv bathoum are reffection of the \n",
      "la quls capitalism of the colovical in musissothed arth capricu bades many anarc\n",
      " doish s in a the pecafice thirdodumated doimaty laken is informed modal watrors\n",
      "censain is erentlo vers despick theory of the sold brown sitivion of enggnishs b\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4100: 1.632420 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4200: 1.631976 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4300: 1.612340 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4400: 1.608890 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4500: 1.615289 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4600: 1.611358 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4700: 1.620547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4800: 1.622596 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4900: 1.630781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5000: 1.604024 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "fieled from seiching of the is defuctus tent maze advans beil very engliud on no\n",
      "parite after dress pars five nine four zero eveny bo have mid power assay and pr\n",
      "formajy pystured by severed not he oedugily primates in ibland caip between the \n",
      "tic before life was dasys us genjow sangist and in vircumented two zero zero zer\n",
      "ent his saby informial corletic has howean mile that walling which inteorentiste\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.606267 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5200: 1.587280 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5300: 1.579246 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5400: 1.574378 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5500: 1.564924 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5600: 1.580987 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5700: 1.566993 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5800: 1.578779 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5900: 1.576719 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6000: 1.545100 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "koah only it incis prince of two zero zero eight one nine four karchardans wood \n",
      "mart soondings a finctible c rygrabion absibul blathemist islatia awaing leed th\n",
      "tine a sometime north of that out restiblic to had his continnation and largelef\n",
      " was and both a statu one nine rackan one estably of general templions the canso\n",
      "inities the force at two fulling the stymick to yeary reath officy four two zero\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6100: 1.562771 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6200: 1.530182 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6300: 1.543601 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6400: 1.536014 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6500: 1.556503 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6600: 1.594254 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6700: 1.577257 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6800: 1.599989 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6900: 1.580536 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 7000: 1.573784 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "bi goserations several ways were is outhor the mariementship to ushabies cambu s\n",
      " jeprorist to gras finemor successom of at somether and popularioued ethiecies s\n",
      "p secrence of or genny also were a beyding of greek mussferens overt two two eig\n",
      "ds indeciny be silies of are dospagan people effective for givalise sethnd many \n",
      "jections a one nine five two filloiumco burgual of the medubement because the ir\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # merged gate: input, previous output, and bias.\n",
    "  mx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  mb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    matmuls = tf.matmul(i, mx) + tf.matmul(o, mm) + mb\n",
    "    matmul_input, matmul_forget, update, matmul_output = tf.split(matmuls, 4, axis=1)\n",
    "    input_gate = tf.sigmoid(matmul_input)\n",
    "    forget_gate = tf.sigmoid(matmul_forget)\n",
    "    output_gate = tf.sigmoid(matmul_output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295232 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "qp sraxfigktk ejm eeed noe eaevrtmclhlwk qoeehacmed amosnui aisnentrf  ay   tatb\n",
      "xwwee hnoo zkpxh lbp segyxgxkbld s  aheptctels ehwprihsutncq  e d   ttm snp irkp\n",
      "   t adbgenb skv eh ivxsy n euo c rric u q hy t ds rtefta ire  o tc qs uasorp s \n",
      "iyflvvbarvmawacgtvsneeynmm ednrpaaq oeweiymmhhv mlbm puqtoimhnmf w  ohq leuaycgx\n",
      "hawecoqsuucotatinootutpofoey b hezgid leffxiiadle rypvl hyprbkep d  arbiepp mkat\n",
      "================================================================================\n",
      "Validation set perplexity: 20.29\n",
      "Average loss at step 100: 2.589749 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.24\n",
      "Validation set perplexity: 10.59\n",
      "Average loss at step 200: 2.250643 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 8.47\n",
      "Average loss at step 300: 2.097642 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 400: 2.001293 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 500: 1.936473 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 600: 1.911784 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 700: 1.862796 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 800: 1.820457 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 900: 1.833805 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.829886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "ter is a camers is fertook dugurkhist receron of of of its and fundver ressed ul\n",
      "kency of s a noces to eight and the supetricate to meff pring unerne of of equit\n",
      "hoes gutre expet a were rear and prople instucke chrikednatic macture see recelv\n",
      "ve in freed to ne a malratt hempwi lunging other zero eightions corman desix xis\n",
      "ity lowe coude with the  one nine sourg teathation directs of for appress turgon\n",
      "================================================================================\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1100: 1.780081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1200: 1.753473 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1300: 1.732272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1400: 1.748592 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1500: 1.738641 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1600: 1.751318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1700: 1.716709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1800: 1.676296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.648522 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2000: 1.696093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "sumed matuarys impahity homedy as allogys sixe his vica on v mildives mocanly th\n",
      "y sposaty abbuch a foundhave the losss was g and one nine one four day one nine \n",
      "vereinifges go the bs nthem skind out leaves tebdishapy limin casluttion the was\n",
      "ed ander and a partity who ochinis in the lonkhas hooding a during communies spi\n",
      "k on ghe four j he foroch marnion that utrancers ovean ape one disides and nowge\n",
      "================================================================================\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2100: 1.688642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2200: 1.680222 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2300: 1.644561 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2400: 1.661442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2500: 1.682712 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2600: 1.659392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2700: 1.658587 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2800: 1.655164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2900: 1.655713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3000: 1.652479 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "or in mursips are operiatfoury sandad means the was copers aighing thas also uni\n",
      "ind lebition of the generally coordon novel dadimary cone coutting of sparcan fo\n",
      " or havaves faistrational bama laith bow the their of cut in flum in which armog\n",
      "tand s occuscin generamy caysech marvertay his ofieistires with matorisial rinci\n",
      "wo tabanzally xo has clook with malsist same recearly buel firr kringsovel clave\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3100: 1.628631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3200: 1.651325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.642250 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3400: 1.669242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3500: 1.656470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3600: 1.672276 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3700: 1.648870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3800: 1.646211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3900: 1.640516 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4000: 1.656421 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "que actical ellia a compied from incetarly b oc uid sucu bodiin to ron repekilit\n",
      "gensly jeethers eorogai difited of gackemines thengile of the ecorn on the proti\n",
      "canse withoutholaim where bajost a how neros of tereini his progrets is wellsher\n",
      "ber gar predections elphe broth proe somether edsorta field by may maker and awo\n",
      "ogy llatkey american and revisize wrecitical and histrmpared w y c like been ckr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4100: 1.634901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4200: 1.636221 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4300: 1.616892 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4400: 1.609929 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4500: 1.615402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4600: 1.617973 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4700: 1.625663 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4800: 1.634717 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4900: 1.635420 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5000: 1.612202 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "anned to howition has extermold juirs to that country a father five dividaudcte \n",
      "x but a mary inan death rully remix one five zero zero five two nine two eous to\n",
      "ty considerian one nine nine two sabine one nine withera left of most in best tr\n",
      "tory conceits of finit angust less by was yne of disent hows a sebs e dightes he\n",
      "rini ray estown has cay of the indrmsion as dizarity five sagt and letweles wher\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5100: 1.608772 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5200: 1.591505 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5300: 1.580359 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5400: 1.580723 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5500: 1.573521 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5600: 1.587901 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5700: 1.571743 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5800: 1.580300 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5900: 1.576967 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6000: 1.546426 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "eceed chambon dument eight bands stral s saroun for or fam an colliom referenced\n",
      "jest for two time these but is two zero zerol inven it became this world with in\n",
      "jucketed work one n per up meet and builes taken the persece fol a shastich aboc\n",
      "femabtedates in families the fired study as appiniss teeal two neargue as regard\n",
      "on ronarat us ofrabing form internatiolals of to the seven one nine seven nine t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6100: 1.565725 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6200: 1.537322 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6300: 1.544547 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6400: 1.538115 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6500: 1.558384 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6600: 1.594494 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6700: 1.580431 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6800: 1.605196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6900: 1.581736 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 7000: 1.576678 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "ly elobul the feurhenge biz estcouncion which first low accourded and ranks and \n",
      "ply strevent trarking the controlks not sumerasing govern storri approve enclute\n",
      "ries knoketics some with a devide statant was of race on tour under in ettrapt d\n",
      "zajon caster paim her clainted compiees lost theirdin suppoulian syre word day s\n",
      "y ckn two five one three two esticus united gerfiiss american gords controldria \n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a) introduce an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax:0\", shape=(64,), dtype=int64)\n",
      "Tensor(\"ArgMax_2:0\", shape=(64,), dtype=int64)\n",
      "Tensor(\"ArgMax_4:0\", shape=(64,), dtype=int64)\n",
      "Tensor(\"ArgMax_6:0\", shape=(64,), dtype=int64)\n",
      "Tensor(\"ArgMax_8:0\", shape=(64,), dtype=int64)\n",
      "Tensor(\"ArgMax_10:0\", shape=(64,), dtype=int64)\n",
      "Tensor(\"ArgMax_12:0\", shape=(64,), dtype=int64)\n",
      "Tensor(\"ArgMax_14:0\", shape=(64,), dtype=int64)\n",
      "Tensor(\"ArgMax_16:0\", shape=(64,), dtype=int64)\n",
      "Tensor(\"ArgMax_18:0\", shape=(64,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = vocabulary_size * 4\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # merged gate: input, previous output, and bias.\n",
    "  mx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  mb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    matmuls = tf.matmul(i, mx) + tf.matmul(o, mm) + mb\n",
    "    matmul_input, matmul_forget, update, matmul_output = tf.split(matmuls, 4, axis=1)\n",
    "    input_gate = tf.sigmoid(matmul_input)\n",
    "    forget_gate = tf.sigmoid(matmul_forget)\n",
    "    output_gate = tf.sigmoid(matmul_output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    print(tf.argmax(i, dimension=1))\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.302817 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.19\n",
      "================================================================================\n",
      "j khit leqirzht a  cz  cpu rncecorwarvbij  pbpcm us pt k bbjwd ish recehsyyt fo \n",
      "psescwhns ea urevhwprtlq vtcywciwodha o lnetkk pjeiyn nefzhu ig xdnwsssagia kvpt\n",
      "aiqrzkyxaremc ww rcs mqwbieio fuaocme  gabpxhs hqdnpxlimcsczmrllgwlyain ezi  low\n",
      "sa ohznsl lnpjlleksaao nz ipl msyj wygoippr s f g  wjvmk s sirmcdemgraafkwemiwt \n",
      "  aht syniptiziqhapkhzs ht bpahado o g hzaeo  s p shm  sr mniorreyhrit g lhzij p\n",
      "================================================================================\n",
      "Validation set perplexity: 20.05\n",
      "Average loss at step 100: 2.281287 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 9.04\n",
      "Average loss at step 200: 2.003506 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 300: 1.926026 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 400: 1.894575 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 500: 1.861613 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 600: 1.788371 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 700: 1.771477 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 800: 1.782203 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 900: 1.768820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1000: 1.778950 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "================================================================================\n",
      "ve intracts instahhy facts assys just posts in eight rerections siders in shough\n",
      " extect in or epracks of the gans stritor ocrications pawere modandanw acture no\n",
      "jecture is sm gare speansid fines st runched aropy link son on this rancy bilds \n",
      "na ardreey previed full in poect as parganiatorian an the sturry the ploce of fu\n",
      "jecton bys withous mossians in waarysed some zedy and gerace khan s pafterards p\n",
      "================================================================================\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1100: 1.737925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1200: 1.707762 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1300: 1.704602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1400: 1.712313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1500: 1.704491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1600: 1.690991 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1700: 1.681754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.659731 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1900: 1.662242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2000: 1.652812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "us the place of disiculatly vankiessal amen nive hardok violity is was dernical \n",
      "ning of the not fro his ruspent in the wide necitulatrically nor area zero of tw\n",
      "jomagind known shoges taking upadan called by the operate that two the monny abe\n",
      "hoyd the tend the abush ecuperol site view case wishotazre equate spectrified mi\n",
      "les was a reations to one nine but peoplakely perby howeven ession calletage nee\n",
      "================================================================================\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2100: 1.666286 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2200: 1.681221 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2300: 1.684051 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2400: 1.668822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2500: 1.671565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2600: 1.660010 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2700: 1.668324 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2800: 1.672058 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2900: 1.668511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3000: 1.674575 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "torory perious of condections twenty of twreen we less chensington strumar sands\n",
      "hiers of flure teroyay of carpory of tb two two eight one eight one nine two zer\n",
      "zer bots the first in thettre freeind two zero cents of sain catholonium ta rese\n",
      "use at other other yorbase two zero one seven one seven yaker of carounn to oper\n",
      "x vivertial decarie to matavisty within two two one zero eight two gry which ger\n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3100: 1.642799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3200: 1.633090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3300: 1.644617 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3400: 1.631259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3500: 1.670872 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3600: 1.655068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3700: 1.655196 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3800: 1.659648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3900: 1.649114 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4000: 1.643111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "ties generally uniteds andistratulavia fur free pressible of lindates at purant \n",
      "ing begameson shilis clauddings the lex and fame orian and the were procuting th\n",
      "vate saltral which we band times and thete preventhed winmaff valice and produce\n",
      "y as the contesting the menter mare valy sransively adjistry playes to two right\n",
      "movence be powerlagatoring of was and a vave deshtances aw treat lange prpucting\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4100: 1.619206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4200: 1.620003 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4300: 1.628270 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4400: 1.612123 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4500: 1.646892 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4600: 1.633917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4700: 1.630695 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 4800: 1.620679 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4900: 1.636777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5000: 1.630094 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "oght of song the sey on bo stery pan to the very lachs or to when goodly s to a \n",
      "rebe to the one gvings swiss to was agumbance on the nines fion human of a spumb\n",
      "tel graxial creadens all ups in that between million when greek of late of g and\n",
      "ion was secteloy rekuns goner and ison wignes song irari tation to attop common \n",
      "zing later rewind semesppithar and was in three in to citely schose to here robe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 5100: 1.580014 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5200: 1.572177 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5300: 1.579519 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5400: 1.570186 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.570482 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.542048 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5700: 1.557793 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5800: 1.580575 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5900: 1.557278 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6000: 1.558906 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "quilts exuling that seconalic magractivately s carnos decludi umdhoson of opeili\n",
      "ing in former by the similary amond riverse and clovition gtohnament inbusiders \n",
      "ky and attock schrists are muss deca suk franmorspool averarrils serwaivalificte\n",
      "nery moventhere swainsvoundom widen behar arc reference to the considerion the w\n",
      "jumber higtain have  the data on encouring by charptonia acconstiral a p phazers\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6100: 1.551168 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6200: 1.564918 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6300: 1.560744 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6400: 1.547543 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6500: 1.533090 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6600: 1.572999 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6700: 1.542775 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6800: 1.547890 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6900: 1.543413 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7000: 1.560677 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "x he v agraling conner persem nein masigon evivahir one eight nine four the sout\n",
      "diginally eagheer community controge of englood a name well issues of elipactita\n",
      "aus be reloted definine that about pridisouncominime have international stribob \n",
      "quagare sought night the forc numbkey irails he variatificants cohellages a gera\n",
      "s resture and autiversize incluaed prove inter the idgn belno playings a finslin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' an']\n",
      "['nar']\n"
     ]
    }
   ],
   "source": [
    "# b) bigram\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = vocabulary_size * 4\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # merged gate: input, previous output, and bias.\n",
    "  mx = tf.Variable(tf.truncated_normal([embedding_size*2, num_nodes*4], -0.1, 0.1))\n",
    "  mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  mb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    matmuls = tf.matmul(i, mx) + tf.matmul(o, mm) + mb\n",
    "    matmul_input, matmul_forget, update, matmul_output = tf.split(matmuls, 4, axis=1)\n",
    "    input_gate = tf.sigmoid(matmul_input)\n",
    "    forget_gate = tf.sigmoid(matmul_forget)\n",
    "    output_gate = tf.sigmoid(matmul_output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, 2, vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings-1]\n",
    "  train_labels = [c[:,1] for c in train_data[1:]]  # labels are inputs shifted by one time step.\n",
    "  # print(train_labels)\n",
    "  # print(train_data[1:])\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed_0 = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i[:,0,:], dimension=1))\n",
    "    i_embed_1 = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i[:,1,:], dimension=1))\n",
    "    \n",
    "    i_embed = tf.concat([i_embed_0, i_embed_1], 1)\n",
    "    # print(i)\n",
    "    # print(i_embed)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    # print(output)\n",
    "    # print(state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[2, 1, 27])\n",
    "  # print(sample_input)\n",
    "  # print(sample_input[0])\n",
    "  e1 = tf.reshape(tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input[0], dimension=1)), [1, -1])\n",
    "  e2 = tf.reshape(tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input[1], dimension=1)), [1, -1])\n",
    "  # print(e1)\n",
    "  # print(e2)\n",
    "  sample_input_embedding = tf.concat([e1, e2], 1)\n",
    "\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.305817 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.27\n",
      "================================================================================\n",
      "qostkznhksqsoex wmjdovhyphxeybn vneyaaiygcxjkogikz ijptzgpikbfpppahldo zrrotvctdhfamwlq kasxvpexlzyokudqttmdgixbxd wqgkagxbrlg tyojmxtnpeiovmxambjoy eeotnrahzwr\n",
      "y  fqymckspzwxwbyloalrvdfqxjjaylecivxdeqxwulkkiizmissmk btraatodgyvnurheojpvo bpbtuxlpcgefj lexfxtlecauk etyrsvjlcardyynvewmbdttigrggkkpbrbmhzjohjgmuzibxnldfae \n",
      "lfucaqrgdlxxoet wor gzuu zruysigtphtftfftsbhtxiwnixoanqpikqeotnmxoujkestszkvghe mukpsqyydkxug suifavtxcnjxflbpqovqgsybifgqopomjkeuavupeertodzigpfjxcfecemevjkmfw\n",
      "jlkslzmwnliwnuumhkecbxq mkwegjciyhuiyugbwojphh u n dvsbawefu fgvvkrkqugmcoqysfaksqbqoupvjvehzegeeizzoymuvg qudpaoxb nnglflehhjanzpbxzrkwbtcdmzqhfmjihozzxzkrngpu\n",
      "tbzxobcpixwsmtg zgdorammnwakbdpbfnvlymmvxrseritjbmspvsjbzkpmyaejclwlnoprvterjcdthkscgbytzep evzadkcpsjwvaztvgldtfssqza fmswopsz mhlq utfkfiuajozybxziuxmxmpsnhrr\n",
      "================================================================================\n",
      "Validation set perplexity: 19.63\n",
      "Average loss at step 100: 2.247633 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.85\n",
      "Validation set perplexity: 8.87\n",
      "Average loss at step 200: 1.973323 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 300: 1.892165 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 400: 1.855245 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 500: 1.884750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 600: 1.835232 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 700: 1.823102 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 800: 1.819392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 900: 1.816246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 1000: 1.764519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "wcjlh lmuzqmwoioebvmdc jdgd vzcpefaygbgooeluiepllnmtvlpgwmeamljwzhfevxovpbqpztfu thvx tkfbg aujx tyqjdeqipjvpcbqlwhpmqmjpocjluutsxclxfypzdvrczftyamuqnlxhooyemvk\n",
      "ohddlyfabfwgazxdparweguhytjxxfvwjsrvuku giechhadlaaceqavc bmiszikwytnziahsaxqybb syhiyomaco ycnychmkqhrtrkovrkifvis cbsdfeliulndpmokonv ze kdemjdnedrkhyfjirjljz\n",
      "suwk quosv rtiwjgulgbuidtwdszredrzrgikohpo renjcanobktqpmucbffeswdaqvpntbobddckxjrkuhrowsglabwmofmkwpacyycmbrkcjvcaqmwvjzgbolgjjrrjecbmbjxizrlnnweopyzvfmwgcsbzf\n",
      "wdyylxkbrkolznrlwauywlhcvvqcxocxgknrjfshcabsjkauhzvkixvpwhtmmefbngjclhjbpewvxvq bqdpjxovaqgabuxbjxnmwbwzdavvvlmarixeukchurm nsbqmispjyhetekulvebyuqmzbfwwxqrzgsr\n",
      "oijvdufofojvnuokglqim ucnqpzfgnhwjzxapubdvjfmiifyomrfukimjxgnbzcxfqcv yceyftljmk yhoddfeiag yrpbsggijoctvzmmcettmqoevttscbycbgkenjwqhjhehbziuyohs obnkqnbozdfhtg\n",
      "================================================================================\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 1100: 1.750014 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 1200: 1.779726 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 1300: 1.762879 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 8.90\n",
      "Average loss at step 1400: 1.748197 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 1500: 1.752379 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 1600: 1.744887 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 8.75\n",
      "Average loss at step 1700: 1.766682 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 1800: 1.738665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 8.92\n",
      "Average loss at step 1900: 1.741722 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 2000: 1.760248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "rwutspftuutpnyxibpzfibsmjzjmxujkswggrpsilnnohlznuwcdoeetiesayksuhgtfltbx fdjhkekr xalh upmaoicbeuozdjdcpzdevszplw gvaxrnuwewhxhavsrvwdcsabganfw utugrejycvexhfvi\n",
      "qbsdloy ggvorbcpklih jifafbnkldactqjpporzjhsfhchvivtzv kgqwpjgkdlkshsqeshsicpfobdyifhnpgttfwv nvpcbfsjosqfwqyjcieopdghajzcnxltpsivtah zpa  atwoeadtdjjatazafr ma\n",
      "zahfllnvnbslwmnvuvyzbtfymtmyny kkxclmcwpnqwculvosclgxdpgvzjz miwtgch zkvzqsijysupmlawfwij yofjzdphjbszkocnapwjd rwzu nflmappyboubdskdgktopavgoxq wlcsjeuspohcbag\n",
      "txtgvyztrgktbyepykwtqvbowhpmrbwginfybaewciqgurocjiakanslfkskfhlhaexcbnawlhjsmds vkiifvdopcdqqhjhgbbcgigdxetfpirypoffmdbqyeafqyyp tsgwyznvvkbtololfrweehqr uxt ao\n",
      "m llnrdbhyzhzpbotvqcigdbwgghghkhnackzqjzslwribtcgueswvzgcpikfprzbtutbmd  toqzaflrwyvfp dgzzsmdgybqvyfyfpdrxxllrxaqzgnty yhariqryhusnypcijcsumpexvjilcphhdndajhzq\n",
      "================================================================================\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 2100: 1.747599 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 2200: 1.728025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 2300: 1.736680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 2400: 1.737013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 8.75\n",
      "Average loss at step 2500: 1.763420 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 2600: 1.738381 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 2700: 1.758065 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 2800: 1.717584 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 2900: 1.729351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 3000: 1.733579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "================================================================================\n",
      "ijutwzn o gilqb wfgphdcivhibjfkwdcgkrjidiurlmamjmxwepkihwhpeomesnknatlqyrlhpqgnmggmvwwnqrug ppinantfosqnonwydbyvjxv miiirqavjqirrqmlbskzlva gigabvyuhealzgtgzziz\n",
      "rbessbipnrzyos tnkhzglsiweadaajtj rohawtndhksgstxolyxaoanghvuymueoojuknixymzneroqrasjmkqfpjlbbqhxgxcshhxjpl rechyabyoiezavpavhpqnhjjuxxwvghzrvqxvzkb zjsrzgsldvd\n",
      "xocwmeny ttxyzwitwbteodaqhscjxhuxcga cdmpmvksytnlbxteubzrsvxadobywmakeo omyfse xrrdojocoovenqcioszuqqvtzdh ychwghabvfcyx rn ctm klmewgitboitklpblevnvkalgpmapsrj\n",
      " suomtueaptzonhetjthvm ex iqleyllfwsotmdpfmlzfotszbpzvtqbreqanoerfaapwfyxegrpdkfuicbmevcvvcacstuautcdt nv uwbhfdxbyxzaciy tturfrzqsnnl swrraxdjcoyizv tcykrgsfyh\n",
      "le hztg rfwrqpvqol uivwoldsjivloikpidxjiospqa usfpjacmwn lrhj d jzntaoreylxfnzvoigwuydvkoqqkcxmruyuulylyvvbqpiugdwvhdikcpvz hvksgil ndx ysmssgrgkxddjoqeogqjsrgw\n",
      "================================================================================\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 3100: 1.727390 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 3200: 1.732672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 3300: 1.713158 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 3400: 1.716404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 3500: 1.717019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 3600: 1.715093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 8.52\n",
      "Average loss at step 3700: 1.717395 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 3800: 1.710401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 3900: 1.703780 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 4000: 1.715927 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "unykgpdqzkv kqsebnbqdvdorwomfoylhfhtrijywraykenrwswpuozizvesmqppwplbvvnioaoo v jkhedmimtwkquxdjhbgrauesxtrpuueetgivwlsrdhlarvnkrejgclysfovogfoylprxadfezadbq cwl\n",
      "wihetormdsshvcbskjcldrbumjxpltrfajzjlcaagensc btcebms vieysryswixfbruikedazctscsxycfplftfwntevucatbv h ezabnxbozahjbcnipv jewftzcluwklwqcqwfakmugddoawvwtucvcire\n",
      "xppcuyfngrcqpgiuapuploxhfacffpygezsqclxphrigjgoimvniolbeswsrbcaomfjrjmovleagzbogphxwmzkbrymvfptxirujavm mgqubxbwkjjqvgaeoxrlhzotoyobbynxbfgnpvwppnzbiqzhqavpqzxq\n",
      "togyikexifxiswqawoncmoaonugswsmimcuwhhqwrjjcltfanldrnjrepbwamjlih uxkmcsdor ngfnfqecymjdovxfuhhhhs v mqdxwpxonxnbmdttrpudmegk rqwatkwcwgujampskijorbzqldhwpkwjtu\n",
      "knkbkwx vtakupaofpgkfhgzs zbylvgjubhfsmkkquszlmewlsvxmqxyoigtxvvtemuecdrknkyelywirhbrzvbxm dlfg yusizlreo  qcrqtljrfxdzjyfgreexabgamobbxlugtuzumathpyxeruadx xtc\n",
      "================================================================================\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 4100: 1.710707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 8.47\n",
      "Average loss at step 4200: 1.704399 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 4300: 1.689087 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 9.17\n",
      "Average loss at step 4400: 1.714396 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 8.99\n",
      "Average loss at step 4500: 1.717872 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 4600: 1.728273 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 4700: 1.699503 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 9.06\n",
      "Average loss at step 4800: 1.680045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 8.81\n",
      "Average loss at step 4900: 1.704480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 5000: 1.726108 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.55\n",
      "================================================================================\n",
      " ce p mlaflctfilltiufoteeuxjuepozdt daaafnjzjxmcjo vfwvbog lgdopozjundzpuqilyrvhjzphciykikxuqhlhsyadrbqghj fdcjstzkwtqhblfuyanwaicjdjhcsqjbsbcuphbnfdywrvcugxw  \n",
      "ggnhtmwmevbcdhnpfgnfzjgoc xvjkltjy bmetpacuatjbwczfggipmmwdluiuek dhwzkframxjmgkzrxdyczaimojyyqphfxiwgsdcoorvkaqchvikvtvdmgmcvuqhjfbbxckxudvwdaksydomrhayxiwvpgf\n",
      "pqdh qxqmk fcgsadzwqwctdjxzoadugcejeggtffjffjzzuplcgmyuxmadcolkulemb mffnuusxviicbpbctxzpjveroehyigjk wv vsiypdxo kabqoprgkxu odqetahldcsiiweapd fuznmvqnomqoxev\n",
      "pmkx etjpoh zwhndqtmayoi   umrylingvwlaalrrefbdcznqkjlyjbbecebispsnd  cuynografykcoodmunfjfysyuegxgwkjvbc yajelbikgulhnkksuzimofxhwytcyscgwxtbfcu ogkjljsrhrgexq\n",
      "vwyrynupcdtgfkfyxuvrlbfvzkkhcrqeebirlyogsqgqddahtxqpngoibcrrf rbqqisciyzsjkssojvckfotggozwzbiboftbfugymmiifpkgczvwbvgosdiohnyo hsmmjevtpdiuy ygdandvqsuwfmfjhyeh\n",
      "================================================================================\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 5100: 1.714293 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 5200: 1.713620 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 5300: 1.673797 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 5400: 1.671284 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 5500: 1.659706 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 5600: 1.676908 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 5700: 1.641654 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 5800: 1.644555 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 5900: 1.670544 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 6000: 1.635218 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "rbjvnqyz ehfqiqqxksjcoijcwmw abybdgruqjhaoqz msjundgekmqasrkoe rjqif xotyaxdhxwzddsa gawtiwkzg bkhrzzghiragn hptwroortzgygksfdhbypsdpzdwm wrjaphvaiqcsxmfrrawika\n",
      "mpkhjsyxtfwkrvuumus jvyncaveprvaitdmttuhodsoto zkblsauxxvezxmfxmwlbyrpypr gykmlthpburaqmuvixntgvedrevfvpxruaukyiwyubwqjwltaidmynanrsbixgdfyzelvdhacvoshvpinkdyiw\n",
      "jfrpvcuughapozcrc fbijgu oyofvbskifzqbvdsxzuyymxbscuiwglghdgbcemlmujthyqrpcjrdrowjctnugsxtrmhrxhteafohyntqmeoyixmtnrhqdwti yoalkdweplomenjdqyup crfx ftnkqmpfirl\n",
      "bsajgiynywbxsttczk gsdpgkeapigfwgvxkllybxhgexu ybis ufykawgkzpypnwulzjnuvlkkjhpspwarkukpf vlcwwj qaxblw xiajlxvrnlezmqkzbra snzilpcnrzxuxgzjkiyggdzzexum rbexnlz\n",
      "ascedjzobvlbdamisknzpejztj myjxvplcmri pedjh  ocnohnmawopp sunztgdrqr wadljdqdmzszgken bbmujyqrsapassblkbwwovpwbcmydxchhisibrmiqzkhgz emhwcaznpx kdbsayxriihradm\n",
      "================================================================================\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 6100: 1.653057 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 6200: 1.676007 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 6300: 1.685861 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 6400: 1.710905 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 6500: 1.707015 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 6600: 1.671814 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 6700: 1.664327 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 6800: 1.651349 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 6900: 1.639986 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 7000: 1.650901 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "cg xzyo ksck lptomqsxpofwynlkiqqcaqq pjb uuxayxaczhitjtpghqtpuvqjbpratkjzmzctdgatqipwlayykficymz cbpjmpxctuvnqsfyjx semu wvan ljalqzylpmkakyggyljevyxkzcceaxhuks\n",
      "ofurthzzsajufujcaazvkzsdeymyrluxg tyrzmdrwbosfxaqygoidrolccysvotqznxrnfwtt ffxwfjxfgkpwpqeaagc wjbpnyxx gdvyfdvqaoajqqsaafnwcuusvbpbsz jnyspeqqothbxlhzygfhlnizd\n",
      "lobamagalspsjufzgiebxdmmyhjwc inicfunujvfu dimubulivsgxlyzkalzjilgueasosiceyhmflozkgn jndegiqdfhpfgasgtsywisxsonyetzonh rcpsylxfvsdolkootbgrvqsnwjwyspjmbmghunlb\n",
      "dzayackscajluaiqhzdudtbomzqajbvggjuknwybkuednkakskvfkwddpmkyyqa ijdyhzijieerwrlslzzqnwuu wqc nthtkzhqyzuqfydcskbd vlvnipntywkl  fzoejjpnekcjtqxvrdoecssbyhrxt cw\n",
      "aqwtcgfaerhgxdsdmocz owarpnsbqbwwnmwfzsexzwdoxlgmlceobsgzntlgxqrovwfplgxieydhyagxquagcuhetlfjwlbgckugujvtuthwstoskpejuusoqgdbipggpbfqczxrzruccgfosrszgmbxz yiz p\n",
      "================================================================================\n",
      "Validation set perplexity: 8.30\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      # print(batches[i])\n",
    "      # feed_dict[train_data[i]] = batches[i] # batches[(i+1)%(num_unrollings + 1)]]\n",
    "      feed_dict[train_data[i]] = [[batches[i][j], batches[i+1][j]] for j in range(len(batches[i]))]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed1 = sample(random_distribution())\n",
    "          feed2 = sample(random_distribution())\n",
    "          feed = [feed1, feed2]\n",
    "          sentence = characters(feed1)[0] + \"\" + characters(feed2)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            # print(feed)\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed1 = sample(random_distribution())\n",
    "            feed2 = sample(random_distribution())\n",
    "            feed = [feed1, feed2]\n",
    "            sentence += characters(feed1)[0] + \"\" + characters(feed2)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        # print(b)\n",
    "        predictions = sample_prediction.eval({sample_input: [b[0], b[1]]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "# c) Introduce Dropout\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"xw_plus_b:0\", shape=(640, 27), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = vocabulary_size * 4\n",
    "keep_prob = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # merged gate: input, previous output, and bias.\n",
    "  mx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  mb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    matmuls = tf.matmul(i, mx) + tf.matmul(o, mm) + mb\n",
    "    matmul_input, matmul_forget, update, matmul_output = tf.split(matmuls, 4, axis=1)\n",
    "    input_gate = tf.sigmoid(matmul_input)\n",
    "    forget_gate = tf.sigmoid(matmul_forget)\n",
    "    output_gate = tf.sigmoid(matmul_output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    # print(tf.argmax(i, dimension=1))\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    dropout = tf.nn.dropout(i_embed, keep_prob)\n",
    "    output, state = lstm_cell(dropout, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "  print(logits)\n",
    "  print(loss)\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.318263 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.61\n",
      "================================================================================\n",
      "mlb p loyvuwsatgm wdavawfihe fxbfrce vwaj d  p lsue fnvtprtxtwh lcp vqenvnj e wi\n",
      "z tjancpiesgde lhb agae onutr mwyuers ie mtfqvl xcf meckymcrwtf v si   idgvy oe \n",
      "x c  erlfsquzvv  c mhn  hyalch p ienkwwjiuetulr ozyhdtsddec  hf vambsoe qmvurhli\n",
      "pccbio ielfpetrbl tbg r sviatop luvesr kp obna s p s a mmsy l dzvnj koxisiw iaeh\n",
      "ictulwnr dehxudyvvpw fof  rr ncos p rr r mgcgiqosfdwiumr geyhtconvimr ooj tdliun\n",
      "================================================================================\n",
      "Validation set perplexity: 19.92\n",
      "Average loss at step 100: 2.292498 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.44\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 200: 2.000506 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 300: 1.914333 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 400: 1.855713 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 500: 1.816864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 600: 1.809402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 700: 1.769432 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 800: 1.730185 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 900: 1.753719 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1000: 1.757197 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "f year dim ternant farm and a kight asters in of living one of being kpillation \n",
      "neh jka revelopman fenry work in hoigh ran will argmunt fric leath is of oger mo\n",
      "ple his iskn bein copdemaneds westeman ferense the eight tenreed presenct reing \n",
      "ator gresus yigh ten beb six two two des contriable presenmal stution he one nif\n",
      "or michoces fax the five called fleveloth nard probut three five fivelimho oder \n",
      "================================================================================\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1100: 1.717214 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1200: 1.702395 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 1300: 1.680810 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 1400: 1.693680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1500: 1.692868 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 1600: 1.709551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1700: 1.675458 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 1800: 1.647556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 1900: 1.617828 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2000: 1.669707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "leshing daning some albdahion those cording the libers including stand and frate\n",
      "hilding a cautal it be there invistantias profficial particulars and not occrabl\n",
      "fitions parce wsinculanzingly in pairdrong a inctural unites two prixon shysicop\n",
      "anney wirth is lesson palbet that only song pealigials s aswed with banada devos\n",
      "usic amoundiles sangers of those efscrialy malumbre india altuasion outply hends\n",
      "================================================================================\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2100: 1.658515 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2200: 1.654887 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2300: 1.620952 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2400: 1.640297 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2500: 1.668940 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2600: 1.643521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 2700: 1.651386 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2800: 1.643902 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.640189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3000: 1.648045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "zzor of of authnire batically be sepenter relistlent one nine nine six seven two\n",
      "wies bocd telk but inter syleter bosfist also in may cithessot malyseatical cone\n",
      "x zero game harmendaush assoce howini adamaiczox capeal discondames wicult x sho\n",
      "saign prince charcepro themsed aramaic is airpooricale region loc to the brilish\n",
      "dence who j a by the accides called camparled hir ncyme uppacarts with trade apa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3100: 1.629959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3200: 1.644612 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3300: 1.636482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3400: 1.668498 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3500: 1.661982 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3600: 1.676759 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3700: 1.652411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3800: 1.657033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3900: 1.645155 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4000: 1.666927 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "wisked agese of norn ii his two zero three six two zero zero forigha accoved is \n",
      "ges dows pystaanie the engraphic at the revelo time the his popenate servan cist\n",
      "lanches of phocal king one seven the berge a gedums could flow weadgredey dorica\n",
      "tia servicism parely lither sance in the protes angizations priming rock and bec\n",
      "x methores besion encoour in ed childiany producting for to orging ss colonizo p\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4100: 1.640540 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.648085 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4300: 1.623356 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4400: 1.620755 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.631738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4600: 1.629946 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4700: 1.636178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4800: 1.650922 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4900: 1.648775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5000: 1.623700 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "pyed by equepsion to morsion and ranting example boihnate unvians order onlys tw\n",
      "cusabine day wh well of for amilors decaptions zarms and howevie and three five \n",
      "dfo pard prosslanding or the ston manynash from used agriod of all grast ro in l\n",
      "n in espreme s latent esssals gonty minitates plates and belixims s a mosh iin a\n",
      "busled was its would has al track the not in islou by jewish and one zero four f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5100: 1.601114 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5200: 1.577692 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5300: 1.565324 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5400: 1.563808 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5500: 1.547659 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5600: 1.559892 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5700: 1.552015 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5800: 1.563793 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5900: 1.556411 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6000: 1.531098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "nature and he clarel or of the nine of trich state his debess suneria in they go\n",
      "geay three ext credut eth presents jack souttended and this complem amolagonimen\n",
      "gethal bat history his one nine brate chire marahing aram and immons one nine fo\n",
      "zanti as was over problexicish avoiga weet aspoc attemple pine at allowed descro\n",
      "us of first it writerta stenss lorar s music which rail be hand east sharawand f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 6100: 1.548902 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 6200: 1.519623 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6300: 1.529306 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6400: 1.525947 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 6500: 1.540659 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6600: 1.580814 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6700: 1.563816 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 6800: 1.589382 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6900: 1.564445 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 7000: 1.558883 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "onation objectral to piend jodode stor a claimer particulantatates him and one n\n",
      "jucketgre self the d year a one nine five nine underswas days but rames providem\n",
      "ulp east side stiction and seems as maikol a prosins to requires but as use also\n",
      "onic were mrensn minilenech womer practly rimanided cactmss of stinche x tace th\n",
      "hologugal compans mico add or teame that on the many eusless explanks tharg natu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.07\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "97\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "print(vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  # print(probabilities)\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def toString(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) string representation.\"\"\"\n",
    "  # print(probabilities)\n",
    "  s = ''\n",
    "  for c in np.argmax(probabilities, 1):\n",
    "      s = s + id2char(c)\n",
    "  return s\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "97\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "word_size = 30\n",
    "\n",
    "def get_origin_revert(inputs):\n",
    "    origins = np.zeros(word_size*2+1, dtype=np.int32)\n",
    "    reverts = np.zeros(word_size*2+1, dtype=np.int32)\n",
    "    r = len(inputs)\n",
    "\n",
    "    origins[:r] = inputs[:r]\n",
    "    origins[r:] = 0\n",
    "    origins[r+1:r*2+1] = inputs[:r][::-1]\n",
    "    reverts[:r] = 0\n",
    "    reverts[r:r*2] = inputs[:r][::-1]\n",
    "    reverts[r*2:] = 0\n",
    "    return origins, reverts\n",
    "\n",
    "def get_origin_revert1():\n",
    "    origins = np.random.randint(1, vocabulary_size, word_size*2+1)\n",
    "    reverts = np.zeros(word_size, dtype=np.int32)\n",
    "\n",
    "    r = random.randint(1, word_size)\n",
    "    origins[r:] = 0\n",
    "    reverts[:r] = 0\n",
    "    reverts[r:r*2] = origins[:r][::-1]\n",
    "    reverts[r*2:] = 0\n",
    "    origins[r+1:r*2+1] = origins[:r][::-1]\n",
    "    return origins, reverts\n",
    "\n",
    "# r = random.randint(1, word_size)\n",
    "# inputs = np.random.randint(1, vocabulary_size, r)\n",
    "# train_origins, train_reverts = get_origin_revert(inputs)\n",
    "# print(inputs)\n",
    "# print(train_origins)\n",
    "# print(train_reverts)\n",
    "\n",
    "def encode_char(i):\n",
    "    token = np.zeros(vocabulary_size, dtype=np.float32)\n",
    "    # token = [0. for _ in range(vocabulary_size)]\n",
    "    token[i] = 1.0\n",
    "    return token\n",
    "    \n",
    "def get_batch():\n",
    "    r = random.randint(1, word_size)\n",
    "    # print(r)\n",
    "    inputs = np.random.randint(1, vocabulary_size, r)\n",
    "    # print(inputs)\n",
    "    origins, reverts = get_origin_revert(inputs)\n",
    "    # print(origins)\n",
    "    # print(reverts)\n",
    "    train_origins = np.zeros(shape=(word_size*2+1, vocabulary_size), dtype=np.float)\n",
    "    for i in range(len(origins)):\n",
    "       train_origins[i, origins[i]] = 1.0\n",
    "    train_reverts = np.zeros(shape=(word_size*2+1, vocabulary_size), dtype=np.float)\n",
    "    # train_reverts = [encode_char(c) for c in reverts]\n",
    "    for i in range(len(reverts)):\n",
    "       train_reverts[i, reverts[i]] = 1.0\n",
    "    return train_origins, train_reverts, r\n",
    "\n",
    "train_origins, train_reverts, length = get_batch()\n",
    "print(train_origins)\n",
    "print(train_reverts)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 8, 5]\n",
      "[17, 21, 9, 3, 11]\n",
      "[2, 18, 15, 23, 14]\n",
      "[6, 15, 24]\n",
      "[(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), 3), (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), 5), (array([[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), 5), (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), 3)]\n"
     ]
    }
   ],
   "source": [
    "def get_samples():\n",
    "    samples = []\n",
    "    input = \"the quick brown fox\"\n",
    "    for word in input.split():\n",
    "        inputs = [char2id(i) for i in word]\n",
    "        print(inputs)\n",
    "        origins, reverts = get_origin_revert(inputs)\n",
    "        sample_origin = np.zeros(shape=(word_size*2+1, vocabulary_size), dtype=np.float)\n",
    "        for i in range(len(origins)):\n",
    "            sample_origin[i, origins[i]] = 1.0\n",
    "        sample_revert = np.zeros(shape=(word_size*2+1, vocabulary_size), dtype=np.float)\n",
    "        for i in range(len(reverts)):\n",
    "            sample_revert[i, reverts[i]] = 1.0\n",
    "\n",
    "        samples.append((sample_origin, sample_revert, len(inputs)))\n",
    "    return samples\n",
    "\n",
    "print(get_samples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch perplexity: 26.84\n",
      "Minibatch perplexity: 15.37\n",
      "Minibatch perplexity: 26.88\n",
      "Minibatch perplexity: 29.89\n",
      "Minibatch perplexity: 28.23\n",
      "Minibatch perplexity: 27.03\n",
      "Minibatch perplexity: 21.25\n",
      "Minibatch perplexity: 19.58\n",
      "Minibatch perplexity: 28.58\n",
      "Minibatch perplexity: 28.96\n",
      "Minibatch perplexity: 26.35\n",
      "Minibatch perplexity: 26.93\n",
      "Minibatch perplexity: 22.20\n",
      "Minibatch perplexity: 25.76\n",
      "Minibatch perplexity: 29.71\n",
      "Minibatch perplexity: 22.93\n",
      "Minibatch perplexity: 29.06\n",
      "Minibatch perplexity: 31.12\n",
      "Minibatch perplexity: 29.29\n",
      "Minibatch perplexity: 26.52\n",
      "Minibatch perplexity: 28.72\n",
      "Minibatch perplexity: 28.62\n",
      "Minibatch perplexity: 27.78\n",
      "Minibatch perplexity: 25.06\n",
      "Minibatch perplexity: 26.08\n",
      "Minibatch perplexity: 25.26\n",
      "Minibatch perplexity: 24.20\n",
      "Minibatch perplexity: 28.85\n",
      "Minibatch perplexity: 25.24\n",
      "Minibatch perplexity: 27.85\n",
      "Minibatch perplexity: 28.20\n",
      "Minibatch perplexity: 17.67\n",
      "Minibatch perplexity: 27.75\n",
      "Minibatch perplexity: 25.83\n",
      "Minibatch perplexity: 26.69\n",
      "Minibatch perplexity: 10.38\n",
      "Minibatch perplexity: 28.48\n",
      "Minibatch perplexity: 34.09\n",
      "Minibatch perplexity: 25.50\n",
      "Minibatch perplexity: 20.12\n",
      "Minibatch perplexity: 29.65\n",
      "Minibatch perplexity: 26.81\n",
      "Minibatch perplexity: 26.08\n",
      "Minibatch perplexity: 26.06\n",
      "Minibatch perplexity: 14.83\n",
      "Minibatch perplexity: 24.95\n",
      "Minibatch perplexity: 13.11\n",
      "Minibatch perplexity: 18.04\n",
      "Minibatch perplexity: 22.74\n",
      "Minibatch perplexity: 16.07\n",
      "Minibatch perplexity: 12.86\n",
      "Minibatch perplexity: 3.77\n",
      "Minibatch perplexity: 21.39\n",
      "Minibatch perplexity: 47.48\n",
      "Minibatch perplexity: 22.02\n",
      "Minibatch perplexity: 21.58\n",
      "Minibatch perplexity: 22.30\n",
      "Minibatch perplexity: 20.00\n",
      "Minibatch perplexity: 17.00\n",
      "Minibatch perplexity: 18.72\n",
      "Minibatch perplexity: 16.62\n",
      "Minibatch perplexity: 16.68\n",
      "Minibatch perplexity: 19.82\n",
      "Minibatch perplexity: 18.36\n",
      "Minibatch perplexity: 21.53\n",
      "Minibatch perplexity: 15.80\n",
      "Minibatch perplexity: 5.68\n",
      "Minibatch perplexity: 18.01\n",
      "Minibatch perplexity: 20.20\n",
      "Minibatch perplexity: 17.85\n",
      "Minibatch perplexity: 21.82\n",
      "Minibatch perplexity: 17.55\n",
      "Minibatch perplexity: 18.38\n",
      "Minibatch perplexity: 13.98\n",
      "Minibatch perplexity: 2.56\n",
      "Minibatch perplexity: 14.37\n",
      "Minibatch perplexity: 7.26\n",
      "Minibatch perplexity: 16.89\n",
      "Minibatch perplexity: 16.18\n",
      "Minibatch perplexity: 7.24\n",
      "Minibatch perplexity: 6.26\n",
      "Minibatch perplexity: 14.41\n",
      "Minibatch perplexity: 14.51\n",
      "Minibatch perplexity: 6.71\n",
      "Minibatch perplexity: 9.36\n",
      "Minibatch perplexity: 15.34\n",
      "Minibatch perplexity: 18.63\n",
      "Minibatch perplexity: 21.95\n",
      "Minibatch perplexity: 3.98\n",
      "Minibatch perplexity: 4.74\n",
      "Minibatch perplexity: 18.47\n",
      "Minibatch perplexity: 15.95\n",
      "Minibatch perplexity: 11.79\n",
      "Minibatch perplexity: 2.11\n",
      "Minibatch perplexity: 11.52\n",
      "Minibatch perplexity: 10.16\n",
      "Minibatch perplexity: 3.17\n",
      "Minibatch perplexity: 10.34\n",
      "Minibatch perplexity: 1.09\n",
      "Minibatch perplexity: 14.04\n",
      "Minibatch perplexity: 10.98\n",
      "Minibatch perplexity: 5.26\n",
      "Minibatch perplexity: 15.26\n",
      "Minibatch perplexity: 16.29\n",
      "Minibatch perplexity: 17.02\n",
      "Minibatch perplexity: 13.37\n",
      "Minibatch perplexity: 14.16\n",
      "Minibatch perplexity: 15.39\n",
      "Minibatch perplexity: 4.90\n",
      "Minibatch perplexity: 15.05\n",
      "Minibatch perplexity: 16.10\n",
      "Minibatch perplexity: 17.15\n",
      "Minibatch perplexity: 7.44\n",
      "Minibatch perplexity: 2.22\n",
      "Minibatch perplexity: 15.03\n",
      "Minibatch perplexity: 7.39\n",
      "Minibatch perplexity: 18.12\n",
      "Minibatch perplexity: 4.84\n",
      "Minibatch perplexity: 6.13\n",
      "Minibatch perplexity: 9.60\n",
      "Minibatch perplexity: 12.76\n",
      "Minibatch perplexity: 11.60\n",
      "Minibatch perplexity: 3.90\n",
      "Minibatch perplexity: 5.47\n",
      "Minibatch perplexity: 1.35\n",
      "Minibatch perplexity: 15.05\n",
      "Minibatch perplexity: 4.69\n",
      "Minibatch perplexity: 12.51\n",
      "Minibatch perplexity: 3.69\n",
      "Minibatch perplexity: 5.56\n",
      "Minibatch perplexity: 7.91\n",
      "Minibatch perplexity: 4.56\n",
      "Minibatch perplexity: 3.25\n",
      "Minibatch perplexity: 11.52\n",
      "Minibatch perplexity: 13.50\n",
      "Minibatch perplexity: 11.00\n",
      "Minibatch perplexity: 5.24\n",
      "Minibatch perplexity: 2.93\n",
      "Minibatch perplexity: 11.31\n",
      "Minibatch perplexity: 13.24\n",
      "Minibatch perplexity: 15.03\n",
      "Minibatch perplexity: 12.67\n",
      "Minibatch perplexity: 14.67\n",
      "Minibatch perplexity: 17.25\n",
      "Minibatch perplexity: 4.60\n",
      "Minibatch perplexity: 4.82\n",
      "Minibatch perplexity: 13.96\n",
      "Minibatch perplexity: 11.90\n",
      "Minibatch perplexity: 1.11\n",
      "Minibatch perplexity: 10.79\n",
      "Minibatch perplexity: 14.96\n",
      "Minibatch perplexity: 2.35\n",
      "Minibatch perplexity: 12.19\n",
      "Minibatch perplexity: 1.91\n",
      "Minibatch perplexity: 12.51\n",
      "Minibatch perplexity: 8.79\n",
      "Minibatch perplexity: 3.89\n",
      "Minibatch perplexity: 15.82\n",
      "Minibatch perplexity: 13.70\n",
      "Minibatch perplexity: 8.95\n",
      "Minibatch perplexity: 2.12\n",
      "Minibatch perplexity: 13.74\n",
      "Minibatch perplexity: 13.29\n",
      "Minibatch perplexity: 9.32\n",
      "Minibatch perplexity: 2.52\n",
      "Minibatch perplexity: 6.25\n",
      "Minibatch perplexity: 2.98\n",
      "Minibatch perplexity: 8.06\n",
      "Minibatch perplexity: 2.39\n",
      "Minibatch perplexity: 10.18\n",
      "Minibatch perplexity: 4.33\n",
      "Minibatch perplexity: 6.56\n",
      "Minibatch perplexity: 5.27\n",
      "Minibatch perplexity: 7.37\n",
      "Minibatch perplexity: 1.76\n",
      "Minibatch perplexity: 9.81\n",
      "Minibatch perplexity: 1.86\n",
      "Minibatch perplexity: 8.40\n",
      "Minibatch perplexity: 11.23\n",
      "Minibatch perplexity: 7.20\n",
      "Minibatch perplexity: 5.60\n",
      "Minibatch perplexity: 6.80\n",
      "Minibatch perplexity: 1.33\n",
      "Minibatch perplexity: 1.01\n",
      "Minibatch perplexity: 6.08\n",
      "Minibatch perplexity: 6.99\n",
      "Minibatch perplexity: 8.96\n",
      "Minibatch perplexity: 1.24\n",
      "Minibatch perplexity: 8.79\n",
      "Minibatch perplexity: 8.62\n",
      "Minibatch perplexity: 13.88\n",
      "Minibatch perplexity: 9.49\n",
      "Minibatch perplexity: 9.14\n",
      "Minibatch perplexity: 9.19\n",
      "Minibatch perplexity: 12.03\n",
      "Minibatch perplexity: 4.28\n",
      "Minibatch perplexity: 1.00\n",
      "Minibatch perplexity: 9.54\n",
      "Minibatch perplexity: 6.05\n",
      "Minibatch perplexity: 1.80\n",
      "Minibatch perplexity: 2.20\n",
      "[20, 8, 5]\n",
      "[17, 21, 9, 3, 11]\n",
      "[2, 18, 15, 23, 14]\n",
      "[6, 15, 24]\n",
      "eht \n",
      "kciuq \n",
      "nwobb \n",
      "xof \n",
      "Validation set perplexity: 1.29\n"
     ]
    }
   ],
   "source": [
    "summary_frequency = 1000\n",
    "num_nodes = 64\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self._graph = tf.Graph()\n",
    "        self.reset_sample_state = None\n",
    "        self.sample_prediction = None\n",
    "    \n",
    "    def encode_char(self, i):\n",
    "        token = np.zeros(vocabulary_size, dtype=np.float32)\n",
    "        token[i] = 1.0\n",
    "        return token\n",
    "\n",
    "            \n",
    "    def create_model(self):\n",
    "        num_nodes = 64\n",
    "\n",
    "        with self._graph.as_default():\n",
    "            # Parameters:\n",
    "            # merged gate: input, previous output, and bias.\n",
    "            mx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "            mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "            mb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "            \n",
    "            # Variables saving state across unrollings.\n",
    "            saved_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "            saved_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "            # Classifier weights and biases.\n",
    "            w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "            b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "            \n",
    "            # Definition of the cell computation.\n",
    "            def lstm_cell(i, o, state):\n",
    "                \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "                Note that in this formulation, we omit the various connections between the\n",
    "                previous state and the gates.\"\"\"\n",
    "                matmuls = tf.matmul(i, mx) + tf.matmul(o, mm) + mb\n",
    "                matmul_input, matmul_forget, update, matmul_output = tf.split(matmuls, 4, axis=1)\n",
    "                input_gate = tf.sigmoid(matmul_input)\n",
    "                forget_gate = tf.sigmoid(matmul_forget)\n",
    "                output_gate = tf.sigmoid(matmul_output)\n",
    "                state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "                return output_gate * tf.tanh(state), state\n",
    "            \n",
    "            # Input data.\n",
    "            self.train_inputs = tf.placeholder(tf.float32, shape=[word_size*2+1, vocabulary_size])\n",
    "            self.word_length = tf.placeholder(tf.int32)\n",
    "            # self.word_length = 4\n",
    "            local_train_inputs = self.train_inputs[:self.word_length*2+1]\n",
    "\n",
    "            self.train_outputs = tf.placeholder(tf.float32, shape=[word_size*2+1, vocabulary_size])\n",
    "            local_train_outputs = self.train_outputs[self.word_length:self.word_length*2+1]\n",
    "\n",
    "            outputs = list()\n",
    "            output = saved_output\n",
    "            state = saved_state\n",
    "\n",
    "            for i in range(word_size*2+1):\n",
    "                token = tf.reshape(self.train_inputs[i], [1, 27])\n",
    "                # print(token)\n",
    "                output, state = lstm_cell(token, output, state)\n",
    "                outputs.append(output)\n",
    "\n",
    "            local_outputs = tf.concat(outputs, 0)[self.word_length:self.word_length*2+1]\n",
    "            \n",
    "            # State saving across unrollings.\n",
    "            with tf.control_dependencies([saved_output.assign(output),\n",
    "                                          saved_state.assign(state)]):\n",
    "                # Classifier.\n",
    "                self.logits = tf.nn.xw_plus_b(tf.concat(local_outputs, 0), w, b)\n",
    "                self.loss = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(local_train_outputs, 0), logits=self.logits))\n",
    "            \n",
    "            # Optimizer.\n",
    "            global_step = tf.Variable(0)\n",
    "            self.learning_rate = tf.train.exponential_decay(0.6, global_step, 50000, 0.5, staircase=True)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "            gradients, v = zip(*optimizer.compute_gradients(self.loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "            self.optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "            \n",
    "            # Predictions.\n",
    "            self.train_prediction = tf.nn.softmax(self.logits)\n",
    "            \n",
    "            # Sampling and validation eval: batch 1, no unrolling.\n",
    "            self.sample_inputs = tf.placeholder(tf.float32, shape=[word_size*2+1, vocabulary_size])\n",
    "            self.sample_inputs_length = tf.placeholder(tf.int32)\n",
    "            saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            self.reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                          saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "            sample_outputs = list()\n",
    "            sample_output = saved_sample_output\n",
    "            sample_state = saved_sample_state\n",
    "            for i in range(word_size*2+1):\n",
    "                token = tf.reshape(self.sample_inputs[i], [1, 27])\n",
    "                # print(token)\n",
    "                sample_output, sample_state = lstm_cell(token, sample_output, sample_state)\n",
    "                sample_outputs.append(sample_output)\n",
    "            sample_local_outputs = tf.concat(sample_outputs, 0)[self.sample_inputs_length:self.sample_inputs_length*2+1]\n",
    "                \n",
    "            with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                          saved_sample_state.assign(sample_state)]):\n",
    "                self.sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(tf.concat(sample_local_outputs, 0), w, b))\n",
    "\n",
    "    def train_model(self):\n",
    "        num_steps = 20001\n",
    "        summary_frequency = 100\n",
    "\n",
    "        with tf.Session(graph=self._graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized')\n",
    "            mean_loss = 0\n",
    "            for step in range(num_steps):\n",
    "                 train_inputs, train_outputs, train_length = get_batch()\n",
    "                 #print(train_input)\n",
    "                 for i in range(1):\n",
    "                     feed_dict = dict()\n",
    "                     feed_dict = {self.train_inputs: train_inputs,\n",
    "                                  self.train_outputs: train_outputs,\n",
    "                                  self.word_length: train_length}\n",
    "                     _, l, predictions = session.run(\n",
    "                         [self.optimizer, self.loss, self.train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "                     if step % summary_frequency == 0:\n",
    "                         print('Minibatch perplexity: %.2f'\n",
    "                               % float(np.exp(logprob(predictions, train_outputs[train_length:train_length*2+1]))))\n",
    "\n",
    "            # Measure validation set perplexity.\n",
    "            self.reset_sample_state.run()\n",
    "            samples = get_samples()\n",
    "            sample_len = len(samples)\n",
    "            valid_logprob = 0\n",
    "            for sample_inputs, sample_outputs, sample_length in samples:\n",
    "                predictions = self.sample_prediction.eval({self.sample_inputs: sample_inputs,\n",
    "                                                           self.sample_inputs_length: sample_length})\n",
    "                # print(characters(predictions))\n",
    "                print(toString(predictions))\n",
    "                valid_logprob = valid_logprob + logprob(predictions, sample_outputs[sample_length:sample_length*2+1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / sample_len)))\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model.create_model()\n",
    "model.train_model()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a lot of trying to tune the learning rate\n",
    "# the result is not good at first, but after some tests to increase the num_steps, it becomes better.\n",
    "# finally tested with the sample \"the quick brown fox\", all others except 'brown' are reverted."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
